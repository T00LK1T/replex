import json
import logging
import os
import pathlib
import platform
import re
from dataclasses import dataclass
from datetime import datetime
from time import sleep
from typing import Annotated

from settings import Environment

BASE_PATH = pathlib.Path(__file__).parent
CODE_PATH = BASE_PATH / "input"
RESULT_PATH = BASE_PATH / "result" / datetime.now().strftime("%Y%m%d-%H%M%S")
LOG_PATH = BASE_PATH / "logs"
CSV_PATH = RESULT_PATH / "csv"
JSON_PATH = RESULT_PATH / "json"
TARGET_WORD_PATH = RESULT_PATH / "words"
SEPARATED_WORD_PATH = TARGET_WORD_PATH / "separated"
NEW_CODE_PATH = RESULT_PATH / "output"

ENSURE_PATH_LIST = [
    CODE_PATH,
    LOG_PATH,
    CSV_PATH,
    JSON_PATH,
    NEW_CODE_PATH,
    SEPARATED_WORD_PATH,
]

ALLOWED_ENCODING = Environment.ALLOWED_ENCODING
SLEEP_BEFORE_GETTING_STARTED = Environment.SLEEP_BEFORE_GETTING_STARTED
REPLACE_MAP = Environment.REPLACE_MAP
EXTENSION = Environment.EXTENSION
PROTECTED_KEYWORDS = Environment.PROTECTED_KEYWORDS
KEYWORD_LIST = list(REPLACE_MAP.keys())
REGEX_PATTERN = r"[ \t\n\r\f\v_();}{.-=*~\\<>,\"/\' []+"

# NOTE: global unique text set
unique_text_set = set()


# NOTE: Î°úÍ∑∏ Ìè¨Îß∑ ÏÑ§Ï†ï
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
stream_handler = logging.StreamHandler()
logger = logging.getLogger(__name__)
logger.setLevel(Environment.LOGGING_LEVEL)
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)


@dataclass
class KeywordMeta:
    """
    ÌÇ§ÏõåÎìú ÏúÑÏπò Ï†ïÎ≥¥Î•º Ï†ÄÏû•Ìï† Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§

    NOTE: ÌÖçÏä§Ìä∏Î∂Å Ï†ïÎ≥¥Î•º Ï†ÄÏû•ÌïòÍ∏∞ÏúÑÌï¥ ÏÇ¨Ïö©ÌïòÍ∏∞ÎèÑ Ìï©ÎãàÎã§.
    ÏûêÎ£åÍµ¨Ï°∞Í∞Ä ÌÅ¨Í≤å Îã§Î•¥ÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê Í∞ôÏù¥ ÏÇ¨Ïö©ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§.
    ÏÇ¨Ïö©Î™©Ï†ÅÏù¥ÎÇò Í∏∞ÌöçÏù¥ Î≥ÄÍ≤ΩÎêòÎäîÍ≤ΩÏö∞ Î∂ÑÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.
    """

    filepath: Annotated[pathlib.Path, "ÌååÏùº Í≤ΩÎ°ú"]
    original_text: Annotated[str, "ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®Îêú ÌÖçÏä§Ìä∏ ÎùºÏù∏"]
    keyword: Annotated[str, "Ï∞æÏùÄ ÌÇ§ÏõåÎìú"]
    line: Annotated[int, "ÎùºÏù∏ Î≤àÌò∏"]
    pos: Annotated[int | None, "Ìï¥Îãπ ÎùºÏù∏Ïùò ÌÇ§ÏõåÎìú ÏãúÏûë ÏúÑÏπò"] = None

    def __str__(self):
        """
        CSV ÌååÏùºÏóê Ï†ÄÏû•ÌïòÍ∏∞ ÏúÑÌï¥ Î¨∏ÏûêÏó¥ Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§.
        """
        return (
            f"{self.filepath.name}\t"
            f"{self.line}\t"
            f"{self.pos}\t"
            f"{self.keyword}\t"
            f"{self.original_text}"
        )

    def __dict__(self):
        """
        JSON ÌòïÏãùÏúºÎ°ú ÏÇ¨Ïö©Ìï†Í≤ΩÏö∞ ÎåÄÏùëÌïòÍ∏∞ ÏúÑÌï¥ ÎîïÏÖîÎÑàÎ¶¨ Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§.
        """
        return {
            "filename": self.filepath.name,
            "line": self.line,
            "pos": self.pos,
            "keyword": self.keyword,
            "original_text": self.original_text,
        }

    def minify(self):
        """
        ÏõêÎ≥∏ ÌÖçÏä§Ìä∏Îßå ÌôïÏù∏ÌïòÍ∏∞ÏúÑÌï¥ ÏÇ¨Ïö©Ìï©ÎãàÎã§.
        """
        return self.original_text


def get_all_paths_with_symlinks(
    directory: pathlib.Path, extensions: list[str]
) -> list[pathlib.Path]:
    all_paths = []
    for root, dirs, files in os.walk(directory, followlinks=True):
        for name in dirs + files:
            full_path = os.path.realpath(os.path.join(root, name))
            path_obj = pathlib.Path(full_path)
            if extensions:  # ÌôïÏû•Ïûê ÌïÑÌÑ∞ÎßÅ
                if path_obj.is_file() and path_obj.suffix[1:] in extensions:
                    all_paths.append(path_obj)
            else:
                if path_obj.is_file():
                    all_paths.append(path_obj)
    return all_paths


def get_relative_target_path(
    filepath: pathlib.Path,
    target_path: pathlib.Path,
) -> pathlib.Path:
    dir_path = filepath.parent

    try:
        relative_path = dir_path.relative_to(BASE_PATH)
        # NOTE: Í≤∞Í≥ºÎîîÎ†âÌÑ∞Î¶¨Ïóê ÏÉùÏÑ±ÎêòÎäî ÎîîÎ†âÌÑ∞Î¶¨ Íµ¨Ï°∞Î•º Îã®ÏàúÌôîÌïòÍ∏∞ÏúÑÌï¥
        # Í∞ÄÏû• ÏÉÅÏúÑÎ†àÎ≤® ÎîîÎ†âÌÑ∞Î¶¨Î•º Ï†úÍ±∞ÌïòÍ∏∞ÏúÑÌïú ÏΩîÎìú ÏûëÏÑ±
        if len(relative_path.parts) > 1:
            relative_path = pathlib.Path(*relative_path.parts[1:])
    except ValueError:
        # NOTE: subpathÍ∞Ä ÏïÑÎãåÍ≤ΩÏö∞
        match platform.system():
            case "Windows":
                absolute_path = dir_path.absolute()
                # NOTE: ÎìúÎùºÏù¥Î∏å Í≤ΩÎ°úÎ•º Ìè¨Ìï®ÌïòÎ©¥ÏÑú ÏΩúÎ°†Ïù¥ Ìè¨Ìï®ÎêòÏñ¥ Í≤ΩÎ°ú Í≥ÑÏÇ∞Ïãú ÏóêÎü¨Î∞úÏÉù
                colon_removed_path = absolute_path.as_posix().replace(":", "")
                relative_path = f"{target_path.as_posix()}/{colon_removed_path}"
            case _:
                relative_path = f"{target_path.as_posix()}/{dir_path.absolute()}"

    relative_target_path = target_path / relative_path / f"{filepath.name}"
    ensure_path_exists(relative_target_path.parent)
    return relative_target_path


def save_log_as_xlsx(
    keyword_meta_list: Annotated[list[KeywordMeta], "ÌÇ§ÏõåÎìú ÏúÑÏπòÏ†ïÎ≥¥ Î™©Î°ù"],
):
    """
    xlsx ÌååÏùºÎ°ú ÌÇ§ÏõåÎìú ÏúÑÏπò Ï†ïÎ≥¥Î•º Ï†ÄÏû•Ìï©ÎãàÎã§.

    Ìå®ÌÇ§ÏßÄ ÏÑ§ÏπòÎ•º ÏïàÎÇ¥Ìï¥ÏïºÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, Í∏∞Îä•ÏúºÎ°ú Ï†úÍ≥µÌïòÏßÄ ÏïäÏùÑ ÏòàÏ†ïÏù¥ÏóàÏäµÎãàÎã§.
    ÌïÑÏöîÌïòÎã§Î©¥ Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÌõÑ ÏÇ¨Ïö©Ìï¥Ï£ºÏÑ∏Ïöî.
    """
    try:
        # pylint: disable=import-outside-toplevel
        import xlsxwriter
    except ImportError:
        logger.error("üî¥ xlsxwriter Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏñ¥ ÏûàÏßÄ ÏïäÏäµÎãàÎã§.")
        logger.info("üü° Ìå®ÌÇ§ÏßÄ ÏÑ§ÏπòÌõÑ ÏÇ¨Ïö©Ìï¥Ï£ºÏÑ∏Ïöî")
        logger.info("\tpip ÏÇ¨Ïö©Ïûê -> pip install xlsxwriter")
        logger.info("\tpoetry ÏÇ¨Ïö©Ïûê -> poetry add xlsxwriter")
        return
    if not keyword_meta_list:
        return
    XLSX_PATH = RESULT_PATH / "xlsx"
    ensure_path_exists(XLSX_PATH)

    filepath = keyword_meta_list[0].filepath
    xlsx_path = get_relative_target_path(
        filepath=filepath,
        target_path=XLSX_PATH,
    )

    workbook = xlsxwriter.Workbook(xlsx_path)
    worksheet = workbook.add_worksheet()
    bold = workbook.add_format({"bold": True})

    worksheet.write("A1", "filename", bold)
    worksheet.write("B1", "line", bold)
    worksheet.write("C1", "pos", bold)
    worksheet.write("D1", "keyword", bold)
    worksheet.write("E1", "original_text", bold)

    for i, keyword_meta in enumerate(keyword_meta_list, start=1):
        worksheet.write(f"A{i}", keyword_meta.filepath.name)
        worksheet.write(f"B{i}", keyword_meta.line)
        worksheet.write(f"C{i}", keyword_meta.pos)
        worksheet.write(f"D{i}", keyword_meta.keyword)
        worksheet.write(f"E{i}", keyword_meta.original_text)

    workbook.close()


def save_log_as_csv(
    keyword_meta_list: Annotated[list[KeywordMeta], "ÌÇ§ÏõåÎìú ÏúÑÏπòÏ†ïÎ≥¥ Î™©Î°ù"],
):
    """
    ÌÇ§ÏõåÎìú ÏúÑÏπò Ï†ïÎ≥¥Î•º CSV ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.
    """
    if not keyword_meta_list:
        return
    filepath = keyword_meta_list[0].filepath
    csv_path = get_relative_target_path(
        filepath=filepath,
        target_path=CSV_PATH,
    )
    csv_path_with_ext = f"{csv_path}.csv"

    # logger.info(csv_path)
    with open(csv_path_with_ext, mode="w", encoding="utf-8") as c:
        HEADER = "filename\tline\tpos\tkeyword\toriginal_text\n"
        c.write(HEADER)
        sorted_keyword_meta_list_by_line = sorted(
            keyword_meta_list, key=lambda x: (x.line, x.pos)
        )
        for keyword_meta in sorted_keyword_meta_list_by_line:
            c.write(f"{keyword_meta}\n")


def save_log_as_json(
    keyword_meta_list: Annotated[list[KeywordMeta], "ÌÇ§ÏõåÎìú ÏúÑÏπòÏ†ïÎ≥¥ Î™©Î°ù"],
):
    """
    ÌÇ§ÏõåÎìú ÏúÑÏπò Ï†ïÎ≥¥Î•º JSON ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.
    """
    if not keyword_meta_list:
        return
    filepath = keyword_meta_list[0].filepath
    json_path = get_relative_target_path(
        filepath=filepath,
        target_path=JSON_PATH,
    )
    json_path_with_ext = f"{json_path}.json"
    line_pos_map = _reform_keyword_meta_as_line_pos_map(keyword_meta_list)
    with open(json_path_with_ext, "w", encoding="utf-8") as json_file:
        json.dump(line_pos_map, json_file, ensure_ascii=False, indent=2)


def save_textbook(
    textbook_list: Annotated[list[KeywordMeta], "ÌÖçÏä§Ìä∏Î∂Å Î™©Î°ù"],
):
    """
    ÌÖçÏä§Ìä∏Î∂Å Ï†ïÎ≥¥Î•º ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.
    """
    if not textbook_list:
        return

    filepath = textbook_list[0].filepath
    textbook_path = get_relative_target_path(
        filepath=filepath,
        target_path=SEPARATED_WORD_PATH,
    )
    textbook_path_with_ext = f"{textbook_path}.txt"
    written_text = []
    with open(textbook_path_with_ext, "w", encoding="utf-8") as f:
        for textbook in textbook_list:
            text = textbook.minify()
            unique_text_set.add(text)
            if text in written_text:
                continue
            f.write(f"{text}\n")
            written_text.append(text)
    logger.debug("üü¢ Í∞Å ÌååÏùºÏùò ÌÖçÏä§Ìä∏Î∂ÅÏùÑ Ï†ÄÏû•ÌñàÏäµÎãàÎã§.")
    logger.debug("\t%s", textbook_path)


def save_unique_textbook():
    """
    Í∏ÄÎ°úÎ≤å ÏÑ†Ïñ∏Îêú unique_text_setÏùÑ Ï∞∏Ï°∞ÌïòÏó¨ ÌÖçÏä§Ìä∏Î∂ÅÏùÑ Ï†ÄÏû•Ìï©ÎãàÎã§.
    """
    if not unique_text_set:
        return
    unique_text_path = TARGET_WORD_PATH / "total.txt"
    with open(unique_text_path, "w", encoding="utf-8") as f:
        for text in unique_text_set:
            f.write(f"{text}\n")
    logger.debug("üü¢ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏Î∂ÅÏùò Í≥†Ïú†Í∞íÎßå Ï∂îÏ∂úÌïòÏó¨ Ï†ÄÏû•ÌñàÏäµÎãàÎã§.")
    logger.debug("\t%s", unique_text_path)


def read_file(
    filepath: Annotated[pathlib.Path, "ÌååÏùº Í≤ΩÎ°ú"],
) -> Annotated[str, "ÌååÏùº ÎÇ¥Ïö©"]:
    """
    ÌååÏùºÏùÑ ÏùΩÏñ¥ÏÑú ÎÇ¥Ïö©ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.
    """
    for encoding in ALLOWED_ENCODING:
        with open(filepath, mode="r", encoding=encoding) as f:
            logger.info("üü° ÌååÏùº ÎÇ¥Ïö© ÏùΩÏùå -> %s ", filepath)
            try:
                return f.read()
            except UnicodeDecodeError:
                logger.debug("üü° %s Ïù∏ÏΩîÎî©ÏúºÎ°ú ÏùΩÍ∏∞ Ïã§Ìå®", encoding)
                continue
    with open(filepath, mode="r", encoding="utf-8", errors="ignore") as f:
        logger.warning("üü° ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î¨∏ÏûêÏó¥ ÌôïÏù∏ -> %s ", filepath)
        logger.warning("üü° Í∏∞Î°ùÎêú ÌååÏùºÍ≥º ÏõêÎ≥∏ÏùÑ ÎåÄÏ°∞Ìï¥Ï£ºÏÑ∏Ïöî")
        return f.read()


def find_similler_words(
    filepath: Annotated[pathlib.Path, "ÌååÏùº Ïù¥Î¶Ñ, redundant"],
    keyword: Annotated[str, "Îß§Ïπ≠ ÎåÄÏÉÅ ÌÇ§ÏõåÎìú"],
    line: Annotated[str, "ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®Îêú Î¨∏ÏûêÏó¥"],
    line_no: Annotated[int, "ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®Îêú ÎùºÏù∏ Î≤àÌò∏"],
) -> list[KeywordMeta]:
    """
    Ï†ïÍ∑úÌëúÌòÑÏãùÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Îß§Ïπ≠ÌïòÏó¨ Î∂ÑÎ¶¨Îêú ÌÇ§ÏõåÎìú Î™©Î°ùÏùÑ Î∞òÌôòÌï©ÎãàÎã§.

    NOTE:
    Îã®ÏàúÌûà ÌÖçÏä§Ìä∏ Î™©Î°ùÏùÑ Î∞òÌôòÌïòÎäîÎç∞ ÏùòÎØ∏Î•º ÎëêÍ∏∞ ÎïåÎ¨∏Ïóê
    posÎ•º Í≥ÑÏÇ∞ÌïòÎäî Î°úÏßÅÏùÄ ÏÉùÎûµÌï©ÎãàÎã§.
    """
    splitted_words = re.split(
        pattern=REGEX_PATTERN,
        string=line,
    )
    textbook_list = []
    for word in splitted_words:
        if not word:
            continue
        if word.find(keyword) != -1:
            logger.debug(
                "\tüü° %s: %s: %s ÌÇ§ÏõåÎìúÏôÄ Ïú†ÏÇ¨Ìïú Îã®Ïñ¥ %s Î•º Ï∞æÏïòÏäµÎãàÎã§.",
                filepath.name,
                line_no,
                keyword,
                word,
            )
            textbook_list.append(
                KeywordMeta(
                    filepath=filepath,
                    original_text=word,
                    keyword=keyword,
                    line=line_no,
                    pos=None,
                )
            )
    return textbook_list


def find_keyword(
    filepath: Annotated[pathlib.Path, "ÌååÏùº Ïù¥Î¶Ñ (redundant)"],
    text: Annotated[str, "ÌååÏùº ÎÇ¥Ïö©"],
    target_keyword: Annotated[str, "Ï∞æÏùÑ ÌÇ§ÏõåÎìú"],
    protected_keywords: Annotated[list[str], "Î≥¥Ìò∏Ìï† ÌÇ§ÏõåÎìú Î¶¨Ïä§Ìä∏"],
) -> Annotated[
    tuple[list[KeywordMeta], list[KeywordMeta]], "ÌÇ§ÏõåÎìú ÏúÑÏπò, ÌÖçÏä§Ìä∏Î∂Å Î™©Î°ù"
]:
    """
    ÌÖçÏä§Ìä∏ÏóêÏÑú ÌÇ§ÏõåÎìúÎ•º Ï∞æÏïÑÏÑú ÏúÑÏπò Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§.

    Ïù¥ Í≥ºÏ†ïÏóêÏÑú protected_keywordsÏóê Ìè¨Ìï®Îêú ÌÇ§ÏõåÎìúÎäî Î¨¥ÏãúÌï©ÎãàÎã§.
    """
    keywords_ = []
    temp_textbook_list_ = []

    lines = text.split("\n")

    for i, line in enumerate(lines, start=1):
        line_ = line
        if target_keyword not in line_:
            continue
        for protected_keyword in protected_keywords:
            if protected_keyword in line_:
                line_ = line_.replace(protected_keyword, " " * len(protected_keyword))
                logger.debug("üü° %s ÌÇ§ÏõåÎìúÎ•º Î≥¥Ìò∏Ìï©ÎãàÎã§.", protected_keyword)
                logger.debug("üü° -> %s", line)
        pos = 0

        temp_textbook_list_.extend(
            find_similler_words(
                filepath=filepath,
                keyword=target_keyword,
                line=line,
                line_no=i,
            )
        )

        while True:
            pos = line_.find(target_keyword, pos)
            if pos == -1:
                break
            keywords_.append(
                KeywordMeta(
                    filepath=filepath,
                    original_text=line,
                    keyword=target_keyword,
                    line=i,
                    pos=pos,
                )
            )
            pos += len(target_keyword)
    return keywords_, temp_textbook_list_


def find_keywords(
    filepath: Annotated[pathlib.Path, "ÌååÏùº Ïù¥Î¶Ñ (redundant)"],
    text: Annotated[str, "ÌååÏùº ÎÇ¥Ïö©"],
    target_keywords: Annotated[list[str], "Ï∞æÏùÑ ÌÇ§ÏõåÎìú Î¶¨Ïä§Ìä∏"],
    protected_keywords: Annotated[list[str], "Î≥¥Ìò∏Ìï† ÌÇ§ÏõåÎìú Î¶¨Ïä§Ìä∏"],
) -> Annotated[
    tuple[list[KeywordMeta], list[KeywordMeta]], "ÌÇ§ÏõåÎìú ÏúÑÏπò, ÌÖçÏä§Ìä∏Î∂Å Î™©Î°ù"
]:
    keywords_ = []
    temp_textbook_list_ = []
    for keyword in target_keywords:
        keywords, textbook_list = find_keyword(
            filepath=filepath,
            text=text,
            target_keyword=keyword,
            protected_keywords=protected_keywords,
        )
        keywords_ += keywords
        temp_textbook_list_ += textbook_list
    return keywords_, temp_textbook_list_


def ensure_path_exists(path: Annotated[pathlib.Path, "ÎîîÎ†âÌÑ∞Î¶¨ Í≤ΩÎ°ú"]):
    """
    Ï†ÑÎã¨Îêú Í≤ΩÎ°úÏùò ÎîîÎ†âÌÑ∞Î¶¨Í∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏúºÎ©¥ ÏÉùÏÑ±Ìï©ÎãàÎã§.
    """
    if not path.exists():
        path.mkdir(parents=True)


def _reform_keyword_meta_as_line_pos_map(
    keyword_meta_list: Annotated[list[KeywordMeta], "ÌÇ§ÏõåÎìú ÏúÑÏπòÏ†ïÎ≥¥ Î™©Î°ù"],
):
    """
    ÌÇ§ÏõåÎìú ÏúÑÏπò Ï†ïÎ≥¥Î•º ÎùºÏù∏Î≥ÑÎ°ú Í∑∏Î£πÌôîÌï©ÎãàÎã§.
    """
    line_pos_map = {}
    for keyword_meta in keyword_meta_list:
        if keyword_meta.line not in line_pos_map:
            line_pos_map[keyword_meta.line] = {}
        if keyword_meta.keyword not in line_pos_map[keyword_meta.line]:
            line_pos_map[keyword_meta.line][keyword_meta.keyword] = []
        line_pos_map[keyword_meta.line][keyword_meta.keyword].append(keyword_meta.pos)
    return line_pos_map


def _replace_keyword(
    text: Annotated[str, "ÏõêÎ≥∏ ÌÖçÏä§Ìä∏"],
    keyword_meta_list: Annotated[list[KeywordMeta], "ÌÇ§ÏõåÎìú ÏúÑÏπòÏ†ïÎ≥¥ Î™©Î°ù"],
):
    line_pos_map = _reform_keyword_meta_as_line_pos_map(
        keyword_meta_list,
    )
    new_lines = []
    for i, line in enumerate(text.split("\n"), 1):
        if i not in line_pos_map:
            new_lines.append(line)
            continue
        accumulated_pos_delta = 0
        target_pos_list = []
        for keyword, pos_list in line_pos_map[i].items():
            for pos in pos_list:
                target_pos_list.append((keyword, pos))
        sorted_target_pos_list = sorted(target_pos_list, key=lambda x: x[1])
        for keyword, pos in sorted_target_pos_list:
            logger.debug("keyword, pos -> %s, %s", keyword, pos)
            pos += accumulated_pos_delta
            # len(REPLACE_MAP[keyword])Îäî Î™®Îëê Îã§Î¶Ñ
            new_keyword = REPLACE_MAP[keyword]
            new_keyword_len = len(new_keyword)
            line = line[:pos] + new_keyword + line[pos + len(keyword):]
            accumulated_pos_delta += new_keyword_len - len(keyword)
        new_lines.append(line)
    return "\n".join(new_lines)


def replace_keyword(
    filepath: Annotated[pathlib.Path, "ÌååÏùº Ïù¥Î¶Ñ"],
    text: Annotated[str, "ÏõêÎ≥∏ ÌÖçÏä§Ìä∏"],
    keyword_meta_list: Annotated[list[KeywordMeta], "ÌÇ§ÏõåÎìú ÏúÑÏπòÏ†ïÎ≥¥ Î™©Î°ù"],
):
    code_path = get_relative_target_path(
        filepath=filepath,
        target_path=NEW_CODE_PATH,
    )
    result = _replace_keyword(text, keyword_meta_list)
    filename = filepath.name
    logger.info("üü° ÌÇ§ÏõåÎìú ÏπòÌôò -> %s", filename)
    with open(code_path, mode="w", encoding="utf-8") as f:
        logger.info("üü¢ ÌååÏùº Ï†ÄÏû• -> %s", code_path)
        f.write(result)


if __name__ == "__main__":
    # NOTE: file ÌòïÏãùÏùò Î°úÍ∑∏Í∞Ä Ï†ÄÏû•ÎêòÍ∏∞ Ïù¥Ï†ÑÏóê ÎîîÎ†âÌÑ∞Î¶¨Í∞Ä Ï°¥Ïû¨Ìï¥ÏïºÌï©ÎãàÎã§.
    _ = [ensure_path_exists(path) for path in ENSURE_PATH_LIST]

    file_handler = logging.FileHandler(
        LOG_PATH / f'text-replacer-{datetime.now().strftime("%Y%m%d-%H%M%S")}.log',
        encoding="utf-8",
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    logger.info("üü¢ ÏûëÏóÖÏùÑ ÏàòÌñâÌï† Í≤ΩÎ°úÎäî %s ÏûÖÎãàÎã§.", CODE_PATH)

    filepath_list = get_all_paths_with_symlinks(CODE_PATH, EXTENSION)

    for filepath_ in filepath_list:
        logger.info("üü¢ ÏûëÏóÖ Î™©Î°ù ÌôïÏù∏ -> %s", filepath_)

    for sec in range(SLEEP_BEFORE_GETTING_STARTED):
        logger.info(
            "üü° %dÏ¥à Îí§Ïóê ÏûëÏóÖÏùÑ ÏãúÏûëÌï©ÎãàÎã§.", SLEEP_BEFORE_GETTING_STARTED - sec
        )
        sleep(1)

    for filepath_ in filepath_list:
        filename_ = filepath_.name
        text_ = read_file(filepath_)
        keyword_meta_list_, textbook_list_ = find_keywords(
            filepath=filepath_,
            text=text_,
            target_keywords=KEYWORD_LIST,
            protected_keywords=PROTECTED_KEYWORDS,
        )
        save_textbook(textbook_list_)
        save_unique_textbook()
        save_log_as_csv(keyword_meta_list_)
        save_log_as_json(keyword_meta_list_)
        # NOTE: ÏóëÏÖÄ Ï†ÄÏû•Í∏∞Îä•Ïù¥ ÌïÑÏöîÌïòÎã§Î©¥ Ï£ºÏÑùÏùÑ Ìï¥Ï†úÌïòÏÑ∏Ïöî.
        # save_log_as_xlsx(keyword_meta_list_)
        logger.debug(
            "üü¢ %s ÌååÏùºÏóêÏÑú ÌÇ§ÏõåÎìú %s Í∞ú Ï∞æÏïòÏäµÎãàÎã§.",
            filename_,
            len(keyword_meta_list_),
        )
        replace_keyword(
            filepath=filepath_,
            text=text_,
            keyword_meta_list=keyword_meta_list_,
        )

    logger.info("üî¥ ÏûëÏóÖÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.")
