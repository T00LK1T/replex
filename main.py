import json
import logging
import os
import pathlib
import platform
import re
from dataclasses import dataclass
from datetime import datetime
from time import sleep
from typing import Annotated

from settings import Environment

BASE_PATH = pathlib.Path(__file__).parent
CODE_PATH = BASE_PATH / "input"
RESULT_PATH = BASE_PATH / "result" / datetime.now().strftime("%Y%m%d-%H%M%S")
LOG_PATH = BASE_PATH / "logs"
CSV_PATH = RESULT_PATH / "csv"
JSON_PATH = RESULT_PATH / "json"
TARGET_WORD_PATH = RESULT_PATH / "words"
SEPARATED_WORD_PATH = TARGET_WORD_PATH / "separated"
NEW_CODE_PATH = RESULT_PATH / "output"

ENSURE_PATH_LIST = [
    CODE_PATH,
    LOG_PATH,
    CSV_PATH,
    JSON_PATH,
    NEW_CODE_PATH,
    SEPARATED_WORD_PATH,
]

ALLOWED_ENCODING = Environment.ALLOWED_ENCODING
SLEEP_BEFORE_GETTING_STARTED = Environment.SLEEP_BEFORE_GETTING_STARTED
REPLACE_MAP = Environment.REPLACE_MAP
EXTENSION = Environment.EXTENSION
PROTECTED_KEYWORDS = Environment.PROTECTED_KEYWORDS
KEYWORD_LIST = list(REPLACE_MAP.keys())
REGEX_PATTERN = r"[ \t\n\r\f\v_();}{.-=*~\\<>,\"/\' []+"

# NOTE: global unique text set
unique_text_set = set()


# NOTE: ë¡œê·¸ í¬ë§· ì„¤ì •
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
stream_handler = logging.StreamHandler()
logger = logging.getLogger(__name__)
logger.setLevel(Environment.LOGGING_LEVEL)
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)


@dataclass
class KeywordMeta:
    """
    í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì €ì¥í•  ë°ì´í„° í´ë˜ìŠ¤

    NOTE: í…ìŠ¤íŠ¸ë¶ ì •ë³´ë¥¼ ì €ì¥í•˜ê¸°ìœ„í•´ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤.
    ìë£Œêµ¬ì¡°ê°€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šê¸° ë•Œë¬¸ì— ê°™ì´ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.
    ì‚¬ìš©ëª©ì ì´ë‚˜ ê¸°íšì´ ë³€ê²½ë˜ëŠ”ê²½ìš° ë¶„ë¦¬í•´ì£¼ì„¸ìš”.
    """

    filepath: Annotated[pathlib.Path, "íŒŒì¼ ê²½ë¡œ"]
    original_text: Annotated[str, "í‚¤ì›Œë“œê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ë¼ì¸"]
    keyword: Annotated[str, "ì°¾ì€ í‚¤ì›Œë“œ"]
    line: Annotated[int, "ë¼ì¸ ë²ˆí˜¸"]
    pos: Annotated[int | None, "í•´ë‹¹ ë¼ì¸ì˜ í‚¤ì›Œë“œ ì‹œì‘ ìœ„ì¹˜"] = None

    def __str__(self):
        """
        CSV íŒŒì¼ì— ì €ì¥í•˜ê¸° ìœ„í•´ ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        return (
            f"{self.filepath.name}\t"
            f"{self.line}\t"
            f"{self.pos}\t"
            f"{self.keyword}\t"
            f"{self.original_text}"
        )

    def __dict__(self):
        """
        JSON í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©í• ê²½ìš° ëŒ€ì‘í•˜ê¸° ìœ„í•´ ë”•ì…”ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        return {
            "filename": self.filepath.name,
            "line": self.line,
            "pos": self.pos,
            "keyword": self.keyword,
            "original_text": self.original_text,
        }

    def minify(self):
        """
        ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ í™•ì¸í•˜ê¸°ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        return self.original_text


def get_all_paths_with_symlinks(
    directory: pathlib.Path, extensions: list[str]
) -> list[pathlib.Path]:
    all_paths = []
    for root, dirs, files in os.walk(directory, followlinks=True):
        for name in dirs + files:
            full_path = os.path.realpath(os.path.join(root, name))
            path_obj = pathlib.Path(full_path)
            if extensions:  # í™•ì¥ì í•„í„°ë§
                if path_obj.is_file() and path_obj.suffix[1:] in extensions:
                    all_paths.append(path_obj)
            else:
                if path_obj.is_file():
                    all_paths.append(path_obj)
    return all_paths


def get_relative_target_path(
    filepath: pathlib.Path,
    target_path: pathlib.Path,
) -> pathlib.Path:
    dir_path = filepath.parent

    try:
        relative_path = dir_path.relative_to(BASE_PATH)
        # NOTE: ê²°ê³¼ë””ë ‰í„°ë¦¬ì— ìƒì„±ë˜ëŠ” ë””ë ‰í„°ë¦¬ êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í•˜ê¸°ìœ„í•´
        # ê°€ì¥ ìƒìœ„ë ˆë²¨ ë””ë ‰í„°ë¦¬ë¥¼ ì œê±°í•˜ê¸°ìœ„í•œ ì½”ë“œ ì‘ì„±
        if len(relative_path.parts) > 1:
            relative_path = pathlib.Path(*relative_path.parts[1:])
    except ValueError:
        # NOTE: subpathê°€ ì•„ë‹Œê²½ìš°
        match platform.system():
            case "Windows":
                absolute_path = dir_path.absolute()
                # NOTE: ë“œë¼ì´ë¸Œ ê²½ë¡œë¥¼ í¬í•¨í•˜ë©´ì„œ ì½œë¡ ì´ í¬í•¨ë˜ì–´ ê²½ë¡œ ê³„ì‚°ì‹œ ì—ëŸ¬ë°œìƒ
                colon_removed_path = absolute_path.as_posix().replace(":", "")
                relative_path = f"{target_path.as_posix()}/{colon_removed_path}"
            case _:
                relative_path = f"{target_path.as_posix()}/{dir_path.absolute()}"

    relative_target_path = target_path / relative_path / f"{filepath.name}"
    ensure_path_exists(relative_target_path.parent)
    return relative_target_path


def save_log_as_xlsx(
    keyword_meta_list: Annotated[list[KeywordMeta], "í‚¤ì›Œë“œ ìœ„ì¹˜ì •ë³´ ëª©ë¡"],
):
    """
    xlsx íŒŒì¼ë¡œ í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

    íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ ì•ˆë‚´í•´ì•¼í•˜ê¸° ë•Œë¬¸ì—, ê¸°ëŠ¥ìœ¼ë¡œ ì œê³µí•˜ì§€ ì•Šì„ ì˜ˆì •ì´ì—ˆìŠµë‹ˆë‹¤.
    í•„ìš”í•˜ë‹¤ë©´ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
    """
    try:
        # pylint: disable=import-outside-toplevel
        import xlsxwriter
    except ImportError:
        logger.error("ğŸ”´ xlsxwriter íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        logger.info("ğŸŸ¡ íŒ¨í‚¤ì§€ ì„¤ì¹˜í›„ ì‚¬ìš©í•´ì£¼ì„¸ìš”")
        logger.info("\tpip ì‚¬ìš©ì -> pip install xlsxwriter")
        logger.info("\tpoetry ì‚¬ìš©ì -> poetry add xlsxwriter")
        return
    if not keyword_meta_list:
        return
    XLSX_PATH = RESULT_PATH / "xlsx"
    ensure_path_exists(XLSX_PATH)

    filepath = keyword_meta_list[0].filepath
    xlsx_path = get_relative_target_path(
        filepath=filepath,
        target_path=XLSX_PATH,
    )

    workbook = xlsxwriter.Workbook(xlsx_path)
    worksheet = workbook.add_worksheet()
    bold = workbook.add_format({"bold": True})

    worksheet.write("A1", "filename", bold)
    worksheet.write("B1", "line", bold)
    worksheet.write("C1", "pos", bold)
    worksheet.write("D1", "keyword", bold)
    worksheet.write("E1", "original_text", bold)

    for i, keyword_meta in enumerate(keyword_meta_list, start=1):
        worksheet.write(f"A{i}", keyword_meta.filepath.name)
        worksheet.write(f"B{i}", keyword_meta.line)
        worksheet.write(f"C{i}", keyword_meta.pos)
        worksheet.write(f"D{i}", keyword_meta.keyword)
        worksheet.write(f"E{i}", keyword_meta.original_text)

    workbook.close()


def save_log_as_csv(
    keyword_meta_list: Annotated[list[KeywordMeta], "í‚¤ì›Œë“œ ìœ„ì¹˜ì •ë³´ ëª©ë¡"],
):
    """
    í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not keyword_meta_list:
        return
    filepath = keyword_meta_list[0].filepath
    csv_path = get_relative_target_path(
        filepath=filepath,
        target_path=CSV_PATH,
    )
    csv_path_with_ext = f"{csv_path}.csv"

    # logger.info(csv_path)
    with open(csv_path_with_ext, mode="w", encoding="utf-8") as c:
        HEADER = "filename\tline\tpos\tkeyword\toriginal_text\n"
        c.write(HEADER)
        sorted_keyword_meta_list_by_line = sorted(
            keyword_meta_list, key=lambda x: (x.line, x.pos)
        )
        for keyword_meta in sorted_keyword_meta_list_by_line:
            c.write(f"{keyword_meta}\n")


def save_log_as_json(
    keyword_meta_list: Annotated[list[KeywordMeta], "í‚¤ì›Œë“œ ìœ„ì¹˜ì •ë³´ ëª©ë¡"],
):
    """
    í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not keyword_meta_list:
        return
    filepath = keyword_meta_list[0].filepath
    json_path = get_relative_target_path(
        filepath=filepath,
        target_path=JSON_PATH,
    )
    json_path_with_ext = f"{json_path}.json"
    line_pos_map = _reform_keyword_meta_as_line_pos_map(keyword_meta_list)
    with open(json_path_with_ext, "w", encoding="utf-8") as json_file:
        json.dump(line_pos_map, json_file, ensure_ascii=False, indent=2)


def save_textbook(
    textbook_list: Annotated[list[KeywordMeta], "í…ìŠ¤íŠ¸ë¶ ëª©ë¡"],
):
    """
    í…ìŠ¤íŠ¸ë¶ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not textbook_list:
        return

    filepath = textbook_list[0].filepath
    textbook_path = get_relative_target_path(
        filepath=filepath,
        target_path=SEPARATED_WORD_PATH,
    )
    textbook_path_with_ext = f"{textbook_path}.txt"
    written_text = []
    with open(textbook_path_with_ext, "w", encoding="utf-8") as f:
        for textbook in textbook_list:
            text = textbook.minify()
            unique_text_set.add(text)
            if text in written_text:
                continue
            f.write(f"{text}\n")
            written_text.append(text)
    logger.debug("ğŸŸ¢ ê° íŒŒì¼ì˜ í…ìŠ¤íŠ¸ë¶ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    logger.debug("\t%s", textbook_path)


def save_unique_textbook():
    """
    ê¸€ë¡œë²Œ ì„ ì–¸ëœ unique_text_setì„ ì°¸ì¡°í•˜ì—¬ í…ìŠ¤íŠ¸ë¶ì„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not unique_text_set:
        return
    unique_text_path = TARGET_WORD_PATH / "total.txt"
    with open(unique_text_path, "w", encoding="utf-8") as f:
        for text in unique_text_set:
            f.write(f"{text}\n")
    logger.debug("ğŸŸ¢ ì „ì²´ í…ìŠ¤íŠ¸ë¶ì˜ ê³ ìœ ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    logger.debug("\t%s", unique_text_path)


def read_file(
    filepath: Annotated[pathlib.Path, "íŒŒì¼ ê²½ë¡œ"],
) -> Annotated[str, "íŒŒì¼ ë‚´ìš©"]:
    """
    íŒŒì¼ì„ ì½ì–´ì„œ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    for encoding in ALLOWED_ENCODING:
        with open(filepath, mode="r", encoding=encoding) as f:
            logger.info("ğŸŸ¡ íŒŒì¼ ë‚´ìš© ì½ìŒ -> %s ", filepath)
            try:
                return f.read()
            except UnicodeDecodeError:
                logger.debug("ğŸŸ¡ %s ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì‹¤íŒ¨", encoding)
                continue
    with open(filepath, mode="r", encoding="utf-8", errors="ignore") as f:
        logger.warning("ğŸŸ¡ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ìì—´ í™•ì¸ -> %s ", filepath)
        logger.warning("ğŸŸ¡ ê¸°ë¡ëœ íŒŒì¼ê³¼ ì›ë³¸ì„ ëŒ€ì¡°í•´ì£¼ì„¸ìš”")
        return f.read()


def find_similler_words(
    filepath: Annotated[pathlib.Path, "íŒŒì¼ ì´ë¦„, redundant"],
    keyword: Annotated[str, "ë§¤ì¹­ ëŒ€ìƒ í‚¤ì›Œë“œ"],
    line: Annotated[str, "í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ìì—´"],
    line_no: Annotated[int, "í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¼ì¸ ë²ˆí˜¸"],
) -> list[KeywordMeta]:
    """
    ì •ê·œí‘œí˜„ì‹ì„ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­í•˜ì—¬ ë¶„ë¦¬ëœ í‚¤ì›Œë“œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    NOTE:
    ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ”ë° ì˜ë¯¸ë¥¼ ë‘ê¸° ë•Œë¬¸ì—
    posë¥¼ ê³„ì‚°í•˜ëŠ” ë¡œì§ì€ ìƒëµí•©ë‹ˆë‹¤.
    """
    splitted_words = re.split(
        pattern=REGEX_PATTERN,
        string=line,
    )
    textbook_list = []
    for word in splitted_words:
        if not word:
            continue
        if word.find(keyword) != -1:
            logger.debug(
                "\tğŸŸ¡ %s: %s: %s í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´ %s ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                filepath.name,
                line_no,
                keyword,
                word,
            )
            textbook_list.append(
                KeywordMeta(
                    filepath=filepath,
                    original_text=word,
                    keyword=keyword,
                    line=line_no,
                    pos=None,
                )
            )
    return textbook_list


def find_keyword(
    filepath: Annotated[pathlib.Path, "íŒŒì¼ ì´ë¦„ (redundant)"],
    text: Annotated[str, "íŒŒì¼ ë‚´ìš©"],
    target_keyword: Annotated[str, "ì°¾ì„ í‚¤ì›Œë“œ"],
    protected_keywords: Annotated[list[str], "ë³´í˜¸í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
) -> Annotated[
    tuple[list[KeywordMeta], list[KeywordMeta]], "í‚¤ì›Œë“œ ìœ„ì¹˜, í…ìŠ¤íŠ¸ë¶ ëª©ë¡"
]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ì„œ ìœ„ì¹˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì´ ê³¼ì •ì—ì„œ protected_keywordsì— í¬í•¨ëœ í‚¤ì›Œë“œëŠ” ë¬´ì‹œí•©ë‹ˆë‹¤.
    """
    keywords_ = []
    temp_textbook_list_ = []

    lines = text.split("\n")

    for i, line in enumerate(lines, start=1):
        line_ = line
        if target_keyword not in line_:
            continue
        for protected_keyword in protected_keywords:
            if protected_keyword in line_:
                line_ = line_.replace(protected_keyword, " " * len(protected_keyword))
                logger.debug("ğŸŸ¡ %s í‚¤ì›Œë“œë¥¼ ë³´í˜¸í•©ë‹ˆë‹¤.", protected_keyword)
                logger.debug("ğŸŸ¡ -> %s", line)
        pos = 0

        temp_textbook_list_.extend(
            find_similler_words(
                filepath=filepath,
                keyword=target_keyword,
                line=line,
                line_no=i,
            )
        )

        while True:
            pos = line_.find(target_keyword, pos)
            if pos == -1:
                break
            keywords_.append(
                KeywordMeta(
                    filepath=filepath,
                    original_text=line,
                    keyword=target_keyword,
                    line=i,
                    pos=pos,
                )
            )
            pos += len(target_keyword)
    return keywords_, temp_textbook_list_


def find_keywords(
    filepath: Annotated[pathlib.Path, "íŒŒì¼ ì´ë¦„ (redundant)"],
    text: Annotated[str, "íŒŒì¼ ë‚´ìš©"],
    target_keywords: Annotated[list[str], "ì°¾ì„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
    protected_keywords: Annotated[list[str], "ë³´í˜¸í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
) -> Annotated[
    tuple[list[KeywordMeta], list[KeywordMeta]], "í‚¤ì›Œë“œ ìœ„ì¹˜, í…ìŠ¤íŠ¸ë¶ ëª©ë¡"
]:
    keywords_ = []
    temp_textbook_list_ = []
    for keyword in target_keywords:
        keywords, textbook_list = find_keyword(
            filepath=filepath,
            text=text,
            target_keyword=keyword,
            protected_keywords=protected_keywords,
        )
        keywords_ += keywords
        temp_textbook_list_ += textbook_list
    return keywords_, temp_textbook_list_


def ensure_path_exists(path: Annotated[pathlib.Path, "ë””ë ‰í„°ë¦¬ ê²½ë¡œ"]):
    """
    ì „ë‹¬ëœ ê²½ë¡œì˜ ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not path.exists():
        path.mkdir(parents=True)


def _reform_keyword_meta_as_line_pos_map(
    keyword_meta_list: Annotated[list[KeywordMeta], "í‚¤ì›Œë“œ ìœ„ì¹˜ì •ë³´ ëª©ë¡"],
):
    """
    í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´ë¥¼ ë¼ì¸ë³„ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    """
    line_pos_map = {}
    for keyword_meta in keyword_meta_list:
        if keyword_meta.line not in line_pos_map:
            line_pos_map[keyword_meta.line] = {}
        if keyword_meta.keyword not in line_pos_map[keyword_meta.line]:
            line_pos_map[keyword_meta.line][keyword_meta.keyword] = []
        line_pos_map[keyword_meta.line][keyword_meta.keyword].append(keyword_meta.pos)
    return line_pos_map


def _replace_keyword(
    text: Annotated[str, "ì›ë³¸ í…ìŠ¤íŠ¸"],
    keyword_meta_list: Annotated[list[KeywordMeta], "í‚¤ì›Œë“œ ìœ„ì¹˜ì •ë³´ ëª©ë¡"],
):
    line_pos_map = _reform_keyword_meta_as_line_pos_map(
        keyword_meta_list,
    )
    new_lines = []
    for i, line in enumerate(text.split("\n"), 1):
        if i not in line_pos_map:
            new_lines.append(line)
            continue
        accumulated_pos_delta = 0
        target_pos_list = []
        for keyword, pos_list in line_pos_map[i].items():
            for pos in pos_list:
                target_pos_list.append((keyword, pos))
        sorted_target_pos_list = sorted(target_pos_list, key=lambda x: x[1])
        for keyword, pos in sorted_target_pos_list:
            logger.debug("keyword, pos -> %s, %s", keyword, pos)
            pos += accumulated_pos_delta
            # len(REPLACE_MAP[keyword])ëŠ” ëª¨ë‘ ë‹¤ë¦„
            new_keyword = REPLACE_MAP[keyword]
            new_keyword_len = len(new_keyword)
            line = line[:pos] + new_keyword + line[pos + len(keyword):]
            accumulated_pos_delta += new_keyword_len - len(keyword)
        new_lines.append(line)
    return "\n".join(new_lines)


def replace_keyword(
    filepath: Annotated[pathlib.Path, "íŒŒì¼ ì´ë¦„"],
    text: Annotated[str, "ì›ë³¸ í…ìŠ¤íŠ¸"],
    keyword_meta_list: Annotated[list[KeywordMeta], "í‚¤ì›Œë“œ ìœ„ì¹˜ì •ë³´ ëª©ë¡"],
):
    code_path = get_relative_target_path(
        filepath=filepath,
        target_path=NEW_CODE_PATH,
    )
    result = _replace_keyword(text, keyword_meta_list)
    filename = filepath.name
    logger.info("ğŸŸ¡ í‚¤ì›Œë“œ ì¹˜í™˜ -> %s", filename)
    with open(code_path, mode="w", encoding="utf-8") as f:
        logger.info("ğŸŸ¢ íŒŒì¼ ì €ì¥ -> %s", code_path)
        f.write(result)


if __name__ == "__main__":
    # NOTE: file í˜•ì‹ì˜ ë¡œê·¸ê°€ ì €ì¥ë˜ê¸° ì´ì „ì— ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•´ì•¼í•©ë‹ˆë‹¤.
    _ = [ensure_path_exists(path) for path in ENSURE_PATH_LIST]

    file_handler = logging.FileHandler(
        LOG_PATH / f'text-replacer-{datetime.now().strftime("%Y%m%d-%H%M%S")}.log',
        encoding="utf-8",
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    logger.info("ğŸŸ¢ ì‘ì—…ì„ ìˆ˜í–‰í•  ê²½ë¡œëŠ” %s ì…ë‹ˆë‹¤.", CODE_PATH)

    filepath_list = get_all_paths_with_symlinks(CODE_PATH, EXTENSION)

    for filepath_ in filepath_list:
        logger.info("ğŸŸ¢ ì‘ì—… ëª©ë¡ í™•ì¸ -> %s", filepath_)

    for sec in range(SLEEP_BEFORE_GETTING_STARTED):
        logger.info(
            "ğŸŸ¡ %dì´ˆ ë’¤ì— ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.", SLEEP_BEFORE_GETTING_STARTED - sec
        )
        sleep(1)

    for filepath_ in filepath_list:
        filename_ = filepath_.name
        text_ = read_file(filepath_)
        keyword_meta_list_, textbook_list_ = find_keywords(
            filepath=filepath_,
            text=text_,
            target_keywords=KEYWORD_LIST,
            protected_keywords=PROTECTED_KEYWORDS,
        )
        save_textbook(textbook_list_)
        save_unique_textbook()
        save_log_as_csv(keyword_meta_list_)
        save_log_as_json(keyword_meta_list_)
        # NOTE: ì—‘ì…€ ì €ì¥ê¸°ëŠ¥ì´ í•„ìš”í•˜ë‹¤ë©´ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
        # save_log_as_xlsx(keyword_meta_list_)
        logger.debug(
            "ğŸŸ¢ %s íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ %s ê°œ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
            filename_,
            len(keyword_meta_list_),
        )
        replace_keyword(
            filepath=filepath_,
            text=text_,
            keyword_meta_list=keyword_meta_list_,
        )

    logger.info("ğŸ”´ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
